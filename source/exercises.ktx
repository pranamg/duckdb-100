< q1
Import DuckDB and check the version (★☆☆)

< h1
hint: import duckdb, duckdb.__version__

< a1
import duckdb
print(duckdb.__version__)

< q2
Create an in-memory DuckDB connection and run a simple SELECT (★☆☆)

< h2
hint: duckdb.connect(), .sql() or .execute()

< a2
import duckdb
con = duckdb.connect()
result = con.sql("SELECT 'Hello, DuckDB!' AS greeting")
print(result)

< q3
Generate a sequence of numbers from 1 to 10 using DuckDB (★☆☆)

< h3
hint: generate_series() or range()

< a3
SELECT * FROM generate_series(1, 10);

-- Alternative using range()
SELECT * FROM range(1, 11);

< q4
Select specific columns from a generated sequence (★☆☆)

< h4
hint: generate_series with column alias

< a4
SELECT generate_series AS num, generate_series * 2 AS doubled
FROM generate_series(1, 5);

< q5
Use SELECT with literal values to create a simple result set (★☆☆)

< h5
hint: SELECT without FROM, or VALUES clause

< a5
SELECT 1 AS id, 'Alice' AS name, 30 AS age
UNION ALL
SELECT 2, 'Bob', 25
UNION ALL
SELECT 3, 'Charlie', 35;

-- Alternative using VALUES
SELECT * FROM (VALUES (1, 'Alice', 30), (2, 'Bob', 25), (3, 'Charlie', 35)) AS t(id, name, age);

< q6
Create a table from a VALUES clause and query it (★☆☆)

< h6
hint: CREATE TABLE ... AS SELECT ... FROM (VALUES ...)

< a6
CREATE TABLE employees AS 
SELECT * FROM (VALUES 
    (1, 'Alice', 'Engineering', 75000),
    (2, 'Bob', 'Sales', 65000),
    (3, 'Charlie', 'Engineering', 80000),
    (4, 'Diana', 'HR', 60000),
    (5, 'Eve', 'Sales', 70000)
) AS t(id, name, department, salary);

SELECT * FROM employees;

< q7
Sort results by a column in descending order (★☆☆)

< h7
hint: ORDER BY ... DESC

< a7
SELECT * FROM employees ORDER BY salary DESC;

< q8
Limit the result set to the first 3 rows (★☆☆)

< h8
hint: LIMIT

< a8
SELECT * FROM employees ORDER BY salary DESC LIMIT 3;

< q9
Skip the first 2 rows and return the next 3 (★☆☆)

< h9
hint: LIMIT ... OFFSET

< a9
SELECT * FROM employees ORDER BY id LIMIT 3 OFFSET 2;

< q10
Select distinct values from a column (★☆☆)

< h10
hint: SELECT DISTINCT

< a10
SELECT DISTINCT department FROM employees;

< q11
Count the total number of rows in a table (★☆☆)

< h11
hint: COUNT(*)

< a11
SELECT COUNT(*) AS total_employees FROM employees;

< q12
Use column aliases to rename output columns (★☆☆)

< h12
hint: AS keyword

< a12
SELECT 
    name AS employee_name, 
    salary AS annual_salary,
    salary / 12 AS monthly_salary
FROM employees;

< q13
Combine two columns into one with string concatenation (★☆☆)

< h13
hint: || or concat()

< a13
SELECT name || ' - ' || department AS employee_info FROM employees;

-- Alternative using concat()
SELECT concat(name, ' works in ', department) AS description FROM employees;

< q14
Use CASE expression to categorize salary levels (★☆☆)

< h14
hint: CASE WHEN ... THEN ... ELSE ... END

< a14
SELECT 
    name,
    salary,
    CASE 
        WHEN salary >= 75000 THEN 'High'
        WHEN salary >= 65000 THEN 'Medium'
        ELSE 'Low'
    END AS salary_level
FROM employees;

< q15
Use COALESCE to handle NULL values (★☆☆)

< h15
hint: COALESCE(column, default_value)

< a15
SELECT 
    name,
    COALESCE(department, 'Unassigned') AS department
FROM (VALUES ('Frank', NULL), ('Grace', 'IT')) AS t(name, department);

< q16
Filter rows where salary is greater than 65000 (★☆☆)

< h16
hint: WHERE clause with comparison operator

< a16
SELECT * FROM employees WHERE salary > 65000;

< q17
Filter rows using multiple conditions with AND (★☆☆)

< h17
hint: WHERE ... AND ...

< a17
SELECT * FROM employees 
WHERE department = 'Engineering' AND salary > 70000;

< q18
Filter rows using OR for alternative conditions (★☆☆)

< h18
hint: WHERE ... OR ...

< a18
SELECT * FROM employees 
WHERE department = 'Sales' OR department = 'HR';

< q19
Use IN clause to match multiple values (★☆☆)

< h19
hint: WHERE column IN (value1, value2, ...)

< a19
SELECT * FROM employees 
WHERE department IN ('Engineering', 'Sales');

< q20
Use BETWEEN to filter a range of values (★☆☆)

< h20
hint: WHERE column BETWEEN low AND high

< a20
SELECT * FROM employees 
WHERE salary BETWEEN 65000 AND 75000;

< q21
Use LIKE for pattern matching with wildcards (★☆☆)

< h21
hint: LIKE '%pattern%', _ for single char

< a21
SELECT * FROM employees WHERE name LIKE 'A%';

-- Names containing 'i'
SELECT * FROM employees WHERE name LIKE '%i%';

-- Names with exactly 3 characters
SELECT * FROM employees WHERE name LIKE '___';

< q22
Use ILIKE for case-insensitive pattern matching (★☆☆)

< h22
hint: ILIKE (DuckDB-specific case-insensitive LIKE)

< a22
SELECT * FROM employees WHERE name ILIKE 'a%';

< q23
Filter rows where a value is NOT in a list (★☆☆)

< h23
hint: NOT IN

< a23
SELECT * FROM employees 
WHERE department NOT IN ('HR', 'Sales');

< q24
Use IS NULL and IS NOT NULL to filter null values (★☆☆)

< h24
hint: IS NULL, IS NOT NULL

< a24
-- Create sample with NULLs
SELECT * FROM (VALUES 
    (1, 'Alice', 100),
    (2, 'Bob', NULL),
    (3, 'Charlie', 150)
) AS t(id, name, bonus)
WHERE bonus IS NOT NULL;

< q25
Use regular expression matching with SIMILAR TO (★☆☆)

< h25
hint: SIMILAR TO or regexp_matches()

< a25
-- Names starting with A or B
SELECT * FROM employees WHERE name SIMILAR TO '(A|B)%';

-- Alternative using regexp_matches
SELECT * FROM employees WHERE regexp_matches(name, '^[AB]');

< q26
Calculate the sum of all salaries (★☆☆)

< h26
hint: SUM()

< a26
SELECT SUM(salary) AS total_salary FROM employees;

< q27
Calculate the average salary (★☆☆)

< h27
hint: AVG()

< a27
SELECT AVG(salary) AS average_salary FROM employees;

< q28
Find the minimum and maximum salaries (★☆☆)

< h28
hint: MIN(), MAX()

< a28
SELECT 
    MIN(salary) AS min_salary,
    MAX(salary) AS max_salary
FROM employees;

< q29
Count employees per department using GROUP BY (★☆☆)

< h29
hint: GROUP BY, COUNT()

< a29
SELECT department, COUNT(*) AS employee_count
FROM employees
GROUP BY department;

< q30
Calculate total and average salary per department (★☆☆)

< h30
hint: GROUP BY with multiple aggregates

< a30
SELECT 
    department,
    COUNT(*) AS num_employees,
    SUM(salary) AS total_salary,
    AVG(salary) AS avg_salary
FROM employees
GROUP BY department;

< q31
Filter grouped results using HAVING (★☆☆)

< h31
hint: HAVING (filters after GROUP BY)

< a31
SELECT department, AVG(salary) AS avg_salary
FROM employees
GROUP BY department
HAVING AVG(salary) > 67000;

< q32
Use GROUP BY with ORDER BY on aggregated column (★☆☆)

< h32
hint: ORDER BY can use aggregate or alias

< a32
SELECT department, SUM(salary) AS total_salary
FROM employees
GROUP BY department
ORDER BY total_salary DESC;

< q33
Calculate multiple statistics in one query (★☆☆)

< h33
hint: Multiple aggregate functions

< a33
SELECT 
    COUNT(*) AS count,
    SUM(salary) AS sum,
    AVG(salary) AS avg,
    MIN(salary) AS min,
    MAX(salary) AS max,
    STDDEV(salary) AS stddev
FROM employees;

< q34
Use FILTER clause with aggregates (DuckDB feature) (★☆☆)

< h34
hint: aggregate() FILTER (WHERE condition)

< a34
SELECT 
    COUNT(*) AS total,
    COUNT(*) FILTER (WHERE department = 'Engineering') AS engineering_count,
    AVG(salary) FILTER (WHERE department = 'Sales') AS sales_avg_salary
FROM employees;

< q35
Calculate running statistics with GROUP BY ALL (★☆☆)

< h35
hint: GROUP BY ALL (groups by all non-aggregated columns)

< a35
SELECT department, name, AVG(salary) AS avg_salary
FROM employees
GROUP BY ALL;

< q36
Use string aggregation with STRING_AGG or LIST (★☆☆)

< h36
hint: STRING_AGG() or LIST()

< a36
SELECT 
    department,
    STRING_AGG(name, ', ' ORDER BY name) AS employee_names
FROM employees
GROUP BY department;

-- Alternative using LIST
SELECT 
    department,
    LIST(name ORDER BY name) AS employee_list
FROM employees
GROUP BY department;

< q37
Calculate percentile values (★☆☆)

< h37
hint: PERCENTILE_CONT, PERCENTILE_DISC

< a37
SELECT 
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY salary) AS median_salary,
    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY salary) AS q1_salary,
    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY salary) AS q3_salary
FROM employees;

< q38
Create two tables and perform an INNER JOIN (★★☆)

< h38
hint: JOIN ... ON

< a38
CREATE TABLE departments AS 
SELECT * FROM (VALUES 
    ('Engineering', 'Building A'),
    ('Sales', 'Building B'),
    ('HR', 'Building C'),
    ('Marketing', 'Building D')
) AS t(name, location);

SELECT e.name, e.salary, d.location
FROM employees e
INNER JOIN departments d ON e.department = d.name;

< q39
Perform a LEFT JOIN to include unmatched rows (★★☆)

< h39
hint: LEFT JOIN

< a39
-- Add an employee with department not in departments table
INSERT INTO employees VALUES (6, 'Frank', 'IT', 72000);

SELECT e.name, e.department, d.location
FROM employees e
LEFT JOIN departments d ON e.department = d.name;

< q40
Perform a RIGHT JOIN (★★☆)

< h40
hint: RIGHT JOIN

< a40
SELECT e.name, d.name AS dept_name, d.location
FROM employees e
RIGHT JOIN departments d ON e.department = d.name;

< q41
Perform a FULL OUTER JOIN (★★☆)

< h41
hint: FULL OUTER JOIN or FULL JOIN

< a41
SELECT e.name, e.department, d.name AS dept_name, d.location
FROM employees e
FULL OUTER JOIN departments d ON e.department = d.name;

< q42
Use CROSS JOIN to generate all combinations (★★☆)

< h42
hint: CROSS JOIN

< a42
SELECT a.x, b.y, a.x * b.y AS product
FROM (SELECT generate_series AS x FROM generate_series(1, 3)) a
CROSS JOIN (SELECT generate_series AS y FROM generate_series(1, 3)) b;

< q43
Perform a self-join to compare rows within the same table (★★☆)

< h43
hint: Join table to itself with different aliases

< a43
-- Find employees who earn more than others in the same department
SELECT 
    e1.name AS employee, 
    e2.name AS earns_less_than,
    e1.department
FROM employees e1
JOIN employees e2 ON e1.department = e2.department AND e1.salary > e2.salary;

< q44
Join multiple tables in a single query (★★☆)

< h44
hint: Chain multiple JOINs

< a44
CREATE TABLE projects AS 
SELECT * FROM (VALUES 
    (1, 'Project Alpha', 'Engineering'),
    (2, 'Project Beta', 'Sales'),
    (3, 'Project Gamma', 'Engineering')
) AS t(id, name, department);

SELECT e.name AS employee, p.name AS project, d.location
FROM employees e
JOIN departments d ON e.department = d.name
JOIN projects p ON e.department = p.department;

< q45
Use USING clause for joins on same-named columns (★★☆)

< h45
hint: JOIN ... USING (column_name)

< a45
-- Rename department column for demonstration
SELECT e.name, e.salary, d.location
FROM employees e
JOIN (SELECT name AS department, location FROM departments) d 
USING (department);

< q46
Perform a NATURAL JOIN (★★☆)

< h46
hint: NATURAL JOIN (joins on all matching column names)

< a46
CREATE TABLE emp_dept AS 
SELECT id, name AS emp_name, department AS dept_name, salary FROM employees;

CREATE TABLE dept_info AS 
SELECT name AS dept_name, location FROM departments;

SELECT * FROM emp_dept NATURAL JOIN dept_info;

< q47
Use anti-join pattern to find non-matching rows (★★☆)

< h47
hint: LEFT JOIN ... WHERE ... IS NULL

< a47
-- Find employees in departments without a location
SELECT e.name, e.department
FROM employees e
LEFT JOIN departments d ON e.department = d.name
WHERE d.name IS NULL;

< q48
Use semi-join pattern with EXISTS (★★☆)

< h48
hint: WHERE EXISTS (subquery)

< a48
-- Find employees who work on projects
SELECT e.name, e.department
FROM employees e
WHERE EXISTS (
    SELECT 1 FROM projects p WHERE p.department = e.department
);

< q49
Combine results with UNION (★★☆)

< h49
hint: UNION removes duplicates, UNION ALL keeps all

< a49
SELECT name, 'employee' AS type FROM employees
UNION
SELECT name, 'department' AS type FROM departments;

-- UNION ALL keeps duplicates
SELECT department FROM employees
UNION ALL
SELECT name FROM departments;

< q50
Use INTERSECT to find common values (★★☆)

< h50
hint: INTERSECT

< a50
SELECT department FROM employees
INTERSECT
SELECT name FROM departments;

< q51
Use EXCEPT to find differences between sets (★★☆)

< h51
hint: EXCEPT

< a51
-- Departments with no employees
SELECT name FROM departments
EXCEPT
SELECT department FROM employees;

< q52
Write a simple subquery in the WHERE clause (★★☆)

< h52
hint: WHERE column = (SELECT ...)

< a52
-- Find employees with above-average salary
SELECT * FROM employees
WHERE salary > (SELECT AVG(salary) FROM employees);

< q53
Use a subquery in the FROM clause (derived table) (★★☆)

< h53
hint: FROM (subquery) AS alias

< a53
SELECT dept_stats.department, dept_stats.avg_salary
FROM (
    SELECT department, AVG(salary) AS avg_salary
    FROM employees
    GROUP BY department
) AS dept_stats
WHERE dept_stats.avg_salary > 67000;

< q54
Use a correlated subquery (★★☆)

< h54
hint: Subquery references outer query

< a54
-- Find employees who earn more than their department average
SELECT e1.name, e1.salary, e1.department
FROM employees e1
WHERE e1.salary > (
    SELECT AVG(e2.salary) 
    FROM employees e2 
    WHERE e2.department = e1.department
);

< q55
Use subquery with IN clause (★★☆)

< h55
hint: WHERE column IN (SELECT ...)

< a55
-- Find employees in departments that have projects
SELECT * FROM employees
WHERE department IN (SELECT department FROM projects);

< q56
Use subquery with ANY/ALL (★★☆)

< h56
hint: > ANY (subquery), > ALL (subquery)

< a56
-- Salary greater than any Sales employee
SELECT * FROM employees
WHERE salary > ANY (SELECT salary FROM employees WHERE department = 'Sales');

-- Salary greater than all Sales employees
SELECT * FROM employees
WHERE salary > ALL (SELECT salary FROM employees WHERE department = 'Sales');

< q57
Write a Common Table Expression (CTE) (★★☆)

< h57
hint: WITH cte_name AS (query)

< a57
WITH dept_avg AS (
    SELECT department, AVG(salary) AS avg_salary
    FROM employees
    GROUP BY department
)
SELECT e.name, e.salary, d.avg_salary
FROM employees e
JOIN dept_avg d ON e.department = d.department
WHERE e.salary > d.avg_salary;

< q58
Use multiple CTEs in a single query (★★☆)

< h58
hint: WITH cte1 AS (...), cte2 AS (...)

< a58
WITH 
dept_stats AS (
    SELECT department, AVG(salary) AS avg_sal, COUNT(*) AS cnt
    FROM employees GROUP BY department
),
high_avg_depts AS (
    SELECT department FROM dept_stats WHERE avg_sal > 67000
)
SELECT e.* FROM employees e
WHERE e.department IN (SELECT department FROM high_avg_depts);

< q59
Use a CTE that references another CTE (★★☆)

< h59
hint: CTEs can reference previously defined CTEs

< a59
WITH 
all_salaries AS (
    SELECT salary FROM employees
),
salary_stats AS (
    SELECT 
        AVG(salary) AS avg_sal,
        STDDEV(salary) AS std_sal
    FROM all_salaries
)
SELECT e.name, e.salary,
    (e.salary - s.avg_sal) / s.std_sal AS z_score
FROM employees e, salary_stats s;

< q60
Use scalar subquery in SELECT (★★☆)

< h60
hint: SELECT (SELECT ... ) AS column

< a60
SELECT 
    name,
    salary,
    (SELECT AVG(salary) FROM employees) AS company_avg,
    salary - (SELECT AVG(salary) FROM employees) AS diff_from_avg
FROM employees;

< q61
Use LATERAL join for row-by-row subquery evaluation (★★☆)

< h61
hint: LATERAL or , LATERAL

< a61
SELECT e.name, e.department, top_earner.max_salary
FROM employees e,
LATERAL (
    SELECT MAX(salary) AS max_salary 
    FROM employees e2 
    WHERE e2.department = e.department
) AS top_earner;

< q62
Add row numbers to results using ROW_NUMBER() (★★☆)

< h62
hint: ROW_NUMBER() OVER (ORDER BY ...)

< a62
SELECT 
    ROW_NUMBER() OVER (ORDER BY salary DESC) AS rank,
    name,
    salary
FROM employees;

< q63
Partition row numbers by a column (★★☆)

< h63
hint: ROW_NUMBER() OVER (PARTITION BY ... ORDER BY ...)

< a63
SELECT 
    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) AS dept_rank,
    name,
    department,
    salary
FROM employees;

< q64
Use RANK() and DENSE_RANK() for ranking with ties (★★☆)

< h64
hint: RANK() skips numbers after ties, DENSE_RANK() doesn't

< a64
SELECT 
    name,
    salary,
    RANK() OVER (ORDER BY salary DESC) AS rank,
    DENSE_RANK() OVER (ORDER BY salary DESC) AS dense_rank
FROM employees;

< q65
Calculate running totals with SUM() window function (★★☆)

< h65
hint: SUM() OVER (ORDER BY ...)

< a65
SELECT 
    name,
    salary,
    SUM(salary) OVER (ORDER BY id) AS running_total
FROM employees;

< q66
Calculate moving average using window frame (★★☆)

< h66
hint: AVG() OVER (ORDER BY ... ROWS BETWEEN ... AND ...)

< a66
SELECT 
    id,
    salary,
    AVG(salary) OVER (
        ORDER BY id 
        ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING
    ) AS moving_avg
FROM employees;

< q67
Use LAG() to access previous row's value (★★☆)

< h67
hint: LAG(column, offset, default) OVER (...)

< a67
SELECT 
    name,
    salary,
    LAG(salary, 1) OVER (ORDER BY id) AS prev_salary,
    salary - LAG(salary, 1, salary) OVER (ORDER BY id) AS salary_diff
FROM employees;

< q68
Use LEAD() to access next row's value (★★☆)

< h68
hint: LEAD(column, offset, default) OVER (...)

< a68
SELECT 
    name,
    salary,
    LEAD(salary, 1) OVER (ORDER BY id) AS next_salary
FROM employees;

< q69
Use FIRST_VALUE() and LAST_VALUE() (★★☆)

< h69
hint: FIRST_VALUE(), LAST_VALUE() with proper frame

< a69
SELECT 
    name,
    department,
    salary,
    FIRST_VALUE(name) OVER (PARTITION BY department ORDER BY salary DESC) AS highest_earner,
    LAST_VALUE(name) OVER (
        PARTITION BY department ORDER BY salary DESC
        ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
    ) AS lowest_earner
FROM employees;

< q70
Use NTH_VALUE() to get a specific row's value (★★☆)

< h70
hint: NTH_VALUE(column, n) OVER (...)

< a70
SELECT 
    name,
    department,
    salary,
    NTH_VALUE(name, 2) OVER (
        PARTITION BY department ORDER BY salary DESC
        ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
    ) AS second_highest_earner
FROM employees;

< q71
Use NTILE() to divide rows into buckets (★★☆)

< h71
hint: NTILE(n) OVER (ORDER BY ...)

< a71
SELECT 
    name,
    salary,
    NTILE(3) OVER (ORDER BY salary) AS salary_tercile
FROM employees;

< q72
Calculate cumulative distribution with CUME_DIST() (★★☆)

< h72
hint: CUME_DIST() OVER (ORDER BY ...)

< a72
SELECT 
    name,
    salary,
    CUME_DIST() OVER (ORDER BY salary) AS cumulative_dist,
    PERCENT_RANK() OVER (ORDER BY salary) AS percent_rank
FROM employees;

< q73
Use window function with FILTER clause (★★☆)

< h73
hint: aggregate FILTER (WHERE ...) OVER (...)

< a73
SELECT 
    name,
    department,
    salary,
    COUNT(*) FILTER (WHERE salary > 65000) OVER (PARTITION BY department) AS high_earners_in_dept
FROM employees;

< q74
Calculate difference from partition average (★★☆)

< h74
hint: value - AVG() OVER (PARTITION BY ...)

< a74
SELECT 
    name,
    department,
    salary,
    AVG(salary) OVER (PARTITION BY department) AS dept_avg,
    salary - AVG(salary) OVER (PARTITION BY department) AS diff_from_dept_avg
FROM employees;

< q75
Use multiple window functions with named windows (★★☆)

< h75
hint: WINDOW w AS (...)

< a75
SELECT 
    name,
    department,
    salary,
    ROW_NUMBER() OVER w AS row_num,
    SUM(salary) OVER w AS running_total,
    AVG(salary) OVER w AS running_avg
FROM employees
WINDOW w AS (PARTITION BY department ORDER BY salary);

< q76
Filter window function results using QUALIFY (★★☆)

< h76
hint: QUALIFY (DuckDB-specific, filters window results)

< a76
-- Get only the highest paid employee per department
SELECT 
    name,
    department,
    salary,
    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) AS rn
FROM employees
QUALIFY rn = 1;

< q77
Create and query a LIST column (★★☆)

< h77
hint: LIST, array syntax [1, 2, 3]

< a77
SELECT 
    [1, 2, 3, 4, 5] AS my_list,
    LIST_VALUE(10, 20, 30) AS another_list;

-- Access elements (1-indexed)
SELECT 
    [1, 2, 3, 4, 5][1] AS first,
    [1, 2, 3, 4, 5][-1] AS last;

< q78
Use LIST functions for manipulation (★★☆)

< h78
hint: list_append, list_concat, list_contains, etc.

< a78
SELECT 
    list_append([1, 2, 3], 4) AS appended,
    list_concat([1, 2], [3, 4]) AS concatenated,
    list_contains([1, 2, 3], 2) AS contains_2,
    list_distinct([1, 2, 2, 3, 3, 3]) AS distinct_list,
    list_sort([3, 1, 4, 1, 5]) AS sorted;

< q79
Use list_transform to apply function to each element (★★☆)

< h79
hint: list_transform(list, lambda)

< a79
SELECT 
    list_transform([1, 2, 3, 4, 5], x -> x * 2) AS doubled,
    list_transform(['hello', 'world'], s -> upper(s)) AS uppercased;

< q80
Use list_filter to select elements (★★☆)

< h80
hint: list_filter(list, lambda)

< a80
SELECT 
    list_filter([1, 2, 3, 4, 5, 6], x -> x % 2 = 0) AS evens,
    list_filter([1, 2, 3, 4, 5], x -> x > 2) AS greater_than_2;

< q81
Use list_reduce for aggregation (★★☆)

< h81
hint: list_reduce(list, lambda)

< a81
SELECT 
    list_reduce([1, 2, 3, 4, 5], (a, b) -> a + b) AS sum,
    list_reduce([1, 2, 3, 4, 5], (a, b) -> a * b) AS product;

< q82
Flatten nested lists with flatten() (★★☆)

< h82
hint: flatten()

< a82
SELECT 
    flatten([[1, 2], [3, 4], [5]]) AS flattened,
    unnest([[1, 2], [3, 4]]) AS unnested;

< q83
Create and query STRUCT columns (★★☆)

< h83
hint: STRUCT, {'key': value} syntax

< a83
SELECT 
    {'name': 'Alice', 'age': 30, 'city': 'NYC'} AS person,
    STRUCT_PACK(x := 1, y := 2, z := 3) AS point;

-- Access fields
SELECT 
    {'name': 'Alice', 'age': 30}.name AS name,
    {'name': 'Alice', 'age': 30}['age'] AS age;

< q84
Use STRUCT functions (★★☆)

< h84
hint: struct_extract, struct_keys, struct_values

< a84
WITH data AS (
    SELECT {'a': 1, 'b': 2, 'c': 3} AS s
)
SELECT 
    struct_extract(s, 'a') AS extract_a,
    struct_keys(s) AS keys,
    struct_values(s) AS values
FROM data;

< q85
Create and use MAP type (★★☆)

< h85
hint: MAP {key: value}, map_from_entries

< a85
SELECT 
    MAP {'a': 1, 'b': 2, 'c': 3} AS my_map,
    map_from_entries([('x', 10), ('y', 20)]) AS from_entries;

-- Access values
SELECT 
    MAP {'a': 1, 'b': 2}['a'] AS value_a,
    element_at(MAP {'a': 1, 'b': 2}, 'b') AS value_b;

< q86
Use MAP functions (★★☆)

< h86
hint: map_keys, map_values, map_entries

< a86
WITH data AS (
    SELECT MAP {'x': 1, 'y': 2, 'z': 3} AS m
)
SELECT 
    map_keys(m) AS keys,
    map_values(m) AS values,
    map_entries(m) AS entries,
    cardinality(m) AS size
FROM data;

< q87
Use UNNEST to expand arrays/lists into rows (★★☆)

< h87
hint: UNNEST(list), FROM unnest(list)

< a87
SELECT unnest([1, 2, 3, 4, 5]) AS num;

-- Unnest multiple arrays in parallel
SELECT 
    unnest(['a', 'b', 'c']) AS letter,
    unnest([1, 2, 3]) AS number;

-- Unnest with ordinality
SELECT * FROM unnest(['a', 'b', 'c']) WITH ORDINALITY AS t(val, idx);

< q88
Create a table with nested types (★★☆)

< h88
hint: Combine STRUCT, LIST in table definition

< a88
CREATE TABLE orders AS 
SELECT * FROM (VALUES 
    (1, 'Alice', [{'product': 'laptop', 'qty': 1}, {'product': 'mouse', 'qty': 2}]),
    (2, 'Bob', [{'product': 'keyboard', 'qty': 1}]),
    (3, 'Charlie', [{'product': 'monitor', 'qty': 2}, {'product': 'cable', 'qty': 5}])
) AS t(order_id, customer, items);

SELECT order_id, customer, unnest(items) AS item FROM orders;

< q89
Use UNION type for heterogeneous data (★★☆)

< h89
hint: UNION(type1, type2, ...)

< a89
CREATE TABLE events (
    id INTEGER,
    data UNION(num INTEGER, str VARCHAR, flag BOOLEAN)
);

INSERT INTO events VALUES 
    (1, 42),
    (2, 'hello'),
    (3, true);

SELECT 
    id, 
    data,
    union_tag(data) AS type_tag
FROM events;

< q90
Use ASOF JOIN for time-series data (★★★)

< h90
hint: ASOF JOIN matches nearest preceding value

< a90
CREATE TABLE stock_prices AS 
SELECT * FROM (VALUES 
    ('2024-01-01 09:00:00'::TIMESTAMP, 100.0),
    ('2024-01-01 10:00:00'::TIMESTAMP, 101.5),
    ('2024-01-01 11:00:00'::TIMESTAMP, 99.0)
) AS t(ts, price);

CREATE TABLE trades AS 
SELECT * FROM (VALUES 
    ('2024-01-01 09:30:00'::TIMESTAMP, 10),
    ('2024-01-01 10:15:00'::TIMESTAMP, 20),
    ('2024-01-01 11:30:00'::TIMESTAMP, 15)
) AS t(ts, quantity);

SELECT t.ts AS trade_time, t.quantity, s.price AS price_at_trade
FROM trades t
ASOF JOIN stock_prices s ON t.ts >= s.ts;

< q91
Use PIVOT to transform rows to columns (★★★)

< h91
hint: PIVOT ... ON ... USING ...

< a91
CREATE TABLE sales AS 
SELECT * FROM (VALUES 
    ('Q1', 'ProductA', 100),
    ('Q1', 'ProductB', 150),
    ('Q2', 'ProductA', 120),
    ('Q2', 'ProductB', 180),
    ('Q3', 'ProductA', 110),
    ('Q3', 'ProductB', 200)
) AS t(quarter, product, amount);

PIVOT sales ON product USING SUM(amount);

< q92
Use UNPIVOT to transform columns to rows (★★★)

< h92
hint: UNPIVOT ... ON ...

< a92
CREATE TABLE quarterly_sales AS 
SELECT * FROM (VALUES 
    ('ProductA', 100, 120, 110),
    ('ProductB', 150, 180, 200)
) AS t(product, Q1, Q2, Q3);

UNPIVOT quarterly_sales ON Q1, Q2, Q3 INTO NAME quarter VALUE amount;

< q93
Use QUALIFY to filter window function results (★★★)

< h93
hint: QUALIFY filters after window functions

< a93
-- Top 2 earners per department
SELECT name, department, salary
FROM employees
QUALIFY ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) <= 2;

-- Employees earning above their department median
SELECT name, department, salary
FROM employees
QUALIFY salary > PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY salary) 
    OVER (PARTITION BY department);

< q94
Use EXCLUDE and REPLACE in SELECT * (★★★)

< h94
hint: SELECT * EXCLUDE (cols), SELECT * REPLACE (expr AS col)

< a94
-- Exclude specific columns
SELECT * EXCLUDE (id) FROM employees;

-- Replace column with expression
SELECT * REPLACE (salary * 1.1 AS salary) FROM employees;

-- Combine both
SELECT * EXCLUDE (id) REPLACE (upper(name) AS name) FROM employees;

< q95
Use COLUMNS expression for dynamic column selection (★★★)

< h95
hint: COLUMNS('pattern'), COLUMNS(*)

< a95
-- Sum all numeric columns
SELECT SUM(COLUMNS(* EXCLUDE (id, name, department))) FROM employees;

-- Apply function to columns matching pattern
SELECT COLUMNS('salary|id')::VARCHAR FROM employees;

< q96
Use GROUPING SETS for multiple groupings (★★★)

< h96
hint: GROUP BY GROUPING SETS ((cols), (cols), ...)

< a96
SELECT 
    department,
    CASE WHEN GROUPING(department) = 1 THEN 'All Departments' ELSE department END AS dept_label,
    SUM(salary) AS total_salary,
    COUNT(*) AS count
FROM employees
GROUP BY GROUPING SETS ((department), ());

< q97
Use ROLLUP for hierarchical aggregation (★★★)

< h97
hint: GROUP BY ROLLUP (col1, col2)

< a97
CREATE TABLE sales_data AS 
SELECT * FROM (VALUES 
    (2023, 'Q1', 'ProductA', 100),
    (2023, 'Q1', 'ProductB', 150),
    (2023, 'Q2', 'ProductA', 120),
    (2024, 'Q1', 'ProductA', 200)
) AS t(year, quarter, product, amount);

SELECT year, quarter, SUM(amount) AS total
FROM sales_data
GROUP BY ROLLUP (year, quarter)
ORDER BY year NULLS LAST, quarter NULLS LAST;

< q98
Use CUBE for all combination aggregations (★★★)

< h98
hint: GROUP BY CUBE (col1, col2)

< a98
SELECT year, quarter, SUM(amount) AS total
FROM sales_data
GROUP BY CUBE (year, quarter)
ORDER BY year NULLS LAST, quarter NULLS LAST;

< q99
Write a recursive CTE (★★★)

< h99
hint: WITH RECURSIVE cte AS (base UNION ALL recursive)

< a99
-- Generate Fibonacci sequence
WITH RECURSIVE fib(n, a, b) AS (
    SELECT 1, 0, 1
    UNION ALL
    SELECT n + 1, b, a + b FROM fib WHERE n < 10
)
SELECT n, a AS fib_number FROM fib;

-- Hierarchical query (org chart)
CREATE TABLE org AS 
SELECT * FROM (VALUES 
    (1, 'CEO', NULL),
    (2, 'CTO', 1),
    (3, 'CFO', 1),
    (4, 'Dev Lead', 2),
    (5, 'Accountant', 3)
) AS t(id, title, manager_id);

WITH RECURSIVE hierarchy AS (
    SELECT id, title, manager_id, 0 AS level, title AS path
    FROM org WHERE manager_id IS NULL
    UNION ALL
    SELECT o.id, o.title, o.manager_id, h.level + 1, h.path || ' > ' || o.title
    FROM org o JOIN hierarchy h ON o.manager_id = h.id
)
SELECT * FROM hierarchy ORDER BY level, id;

< q100
Read a CSV file with DuckDB (★★☆)

< h100
hint: read_csv_auto(), read_csv()

< a100
-- Auto-detect CSV format
SELECT * FROM read_csv_auto('data.csv');

-- With explicit options
SELECT * FROM read_csv('data.csv', 
    delim = ',',
    header = true,
    columns = {'id': 'INTEGER', 'name': 'VARCHAR', 'value': 'DOUBLE'}
);

-- Read from URL (requires httpfs extension)
-- INSTALL httpfs; LOAD httpfs;
-- SELECT * FROM read_csv_auto('https://example.com/data.csv');

< q101
Write query results to a CSV file (★★☆)

< h101
hint: COPY ... TO ..., or write_csv()

< a101
-- Using COPY
COPY (SELECT * FROM employees) TO 'employees.csv' (HEADER, DELIMITER ',');

-- Using write_csv function (DuckDB 0.9+)
COPY (SELECT * FROM employees) TO 'employees.csv' WITH (FORMAT CSV, HEADER TRUE);

< q102
Read and write Parquet files (★★☆)

< h102
hint: read_parquet(), COPY TO ... (FORMAT PARQUET)

< a102
-- Read Parquet file
SELECT * FROM read_parquet('data.parquet');

-- Read multiple Parquet files with glob
SELECT * FROM read_parquet('data/*.parquet');

-- Write to Parquet
COPY (SELECT * FROM employees) TO 'employees.parquet' (FORMAT PARQUET);

-- With compression
COPY (SELECT * FROM employees) TO 'employees.parquet' (FORMAT PARQUET, COMPRESSION ZSTD);

< q103
Read JSON data (★★☆)

< h103
hint: read_json_auto(), read_json()

< a103
-- Read JSON file
SELECT * FROM read_json_auto('data.json');

-- Read JSON with explicit schema
SELECT * FROM read_json('data.json',
    columns = {'id': 'INTEGER', 'name': 'VARCHAR', 'tags': 'VARCHAR[]'}
);

-- Read newline-delimited JSON
SELECT * FROM read_ndjson_auto('data.ndjson');

< q104
Query remote files using httpfs extension (★★☆)

< h104
hint: INSTALL httpfs; LOAD httpfs;

< a104
-- Install and load httpfs
INSTALL httpfs;
LOAD httpfs;

-- Query CSV from URL
SELECT * FROM read_csv_auto('https://raw.githubusercontent.com/datasets/covid-19/main/data/countries-aggregated.csv') LIMIT 10;

-- Query Parquet from S3 (with credentials)
-- SET s3_region='us-east-1';
-- SET s3_access_key_id='your_key';
-- SET s3_secret_access_key='your_secret';
-- SELECT * FROM read_parquet('s3://bucket/path/file.parquet');

< q105
Use glob patterns to read multiple files (★★☆)

< h105
hint: read_csv_auto('path/*.csv'), read_parquet('path/**/*.parquet')

< a105
-- Read all CSV files in directory
SELECT * FROM read_csv_auto('data/*.csv');

-- Read all Parquet files recursively
SELECT * FROM read_parquet('data/**/*.parquet');

-- Add filename column
SELECT *, filename FROM read_csv_auto('data/*.csv', filename=true);

< q106
Create a view over external files (★★☆)

< h106
hint: CREATE VIEW ... AS SELECT FROM read_...

< a106
-- Create view over CSV file
CREATE VIEW external_data AS 
SELECT * FROM read_csv_auto('data.csv');

-- Query the view
SELECT * FROM external_data WHERE id > 5;

-- Create view over Parquet with filter pushdown
CREATE VIEW parquet_data AS 
SELECT * FROM read_parquet('data.parquet');

< q107
Export database or tables (★★☆)

< h107
hint: EXPORT DATABASE, COPY

< a107
-- Export entire database to directory
EXPORT DATABASE 'backup_dir';

-- Export specific table to Parquet
COPY employees TO 'employees_backup.parquet' (FORMAT PARQUET);

-- Import database
-- IMPORT DATABASE 'backup_dir';

< q108
Use DuckDB with partitioned data (★★☆)

< h108
hint: hive_partitioning, write partitioned data

< a108
-- Read hive-partitioned data
SELECT * FROM read_parquet('data/*/*.parquet', hive_partitioning=true);

-- Write partitioned data
COPY (SELECT * FROM employees) 
TO 'output' (FORMAT PARQUET, PARTITION_BY (department));

< q109
Insert data from external files (★★☆)

< h109
hint: INSERT INTO ... SELECT FROM read_...

< a109
-- Insert from CSV
CREATE TABLE imported_data (id INTEGER, name VARCHAR, value DOUBLE);
INSERT INTO imported_data SELECT * FROM read_csv_auto('data.csv');

-- Insert from Parquet with transformation
INSERT INTO employees 
SELECT id, name, dept, salary * 1.05 
FROM read_parquet('new_employees.parquet');

< q110
Write a simple macro (user-defined function) (★★★)

< h110
hint: CREATE MACRO name(args) AS expression

< a110
-- Simple expression macro
CREATE MACRO double(x) AS x * 2;
SELECT double(5);

-- Macro with multiple arguments
CREATE MACRO full_name(first, last) AS first || ' ' || last;
SELECT full_name('John', 'Doe');

-- Table macro
CREATE MACRO high_earners(min_salary) AS TABLE 
    SELECT * FROM employees WHERE salary > min_salary;
SELECT * FROM high_earners(70000);

< q111
Use the json extension for JSON processing (★★★)

< h111
hint: json_extract, json_extract_string, ->>, etc.

< a111
-- Extract JSON fields
SELECT 
    json_extract('{"name": "Alice", "age": 30}', '$.name') AS name_json,
    json_extract_string('{"name": "Alice", "age": 30}', '$.name') AS name_str,
    '{"name": "Alice", "age": 30}'::JSON ->> 'name' AS name_arrow;

-- Extract nested JSON
SELECT 
    json_extract('{"person": {"name": "Alice", "city": "NYC"}}', '$.person.name') AS nested_name;

-- JSON array handling
SELECT 
    json_extract('[1, 2, 3, 4, 5]', '$[0]') AS first,
    json_array_length('[1, 2, 3, 4, 5]') AS len;

< q112
Use full-text search extension (★★★)

< h112
hint: INSTALL fts; LOAD fts; PRAGMA create_fts_index

< a112
INSTALL fts;
LOAD fts;

-- Create table with text
CREATE TABLE documents AS 
SELECT * FROM (VALUES 
    (1, 'DuckDB is an in-process SQL OLAP database'),
    (2, 'Python is a popular programming language'),
    (3, 'SQL databases store structured data')
) AS t(id, content);

-- Create full-text search index
PRAGMA create_fts_index('documents', 'id', 'content');

-- Search using FTS
SELECT id, content, score
FROM (
    SELECT *, fts_main_documents.match_bm25(id, 'SQL database') AS score
    FROM documents
) sq
WHERE score IS NOT NULL
ORDER BY score DESC;

< q113
Use window functions with complex frames (★★★)

< h113
hint: ROWS/RANGE BETWEEN ... AND ...

< a113
-- Rows-based frame: specific number of rows
SELECT 
    id,
    salary,
    AVG(salary) OVER (ORDER BY id ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS avg_5_rows
FROM employees;

-- Range-based frame: value range
SELECT 
    id,
    salary,
    COUNT(*) OVER (ORDER BY salary RANGE BETWEEN 5000 PRECEDING AND 5000 FOLLOWING) AS similar_salary_count
FROM employees;

-- Exclude current row
SELECT 
    id,
    salary,
    AVG(salary) OVER (ORDER BY id ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING EXCLUDE CURRENT ROW) AS avg_others
FROM employees;

< q114
Use SAMPLE to randomly sample data (★★★)

< h114
hint: USING SAMPLE n, USING SAMPLE n%

< a114
-- Sample fixed number of rows
SELECT * FROM employees USING SAMPLE 3;

-- Sample percentage
SELECT * FROM employees USING SAMPLE 50%;

-- Reproducible sampling with seed
SELECT * FROM employees USING SAMPLE 50% (bernoulli, 42);

-- Reservoir sampling
SELECT * FROM generate_series(1, 1000) USING SAMPLE reservoir(10 ROWS);

< q115
Use EXPLAIN and EXPLAIN ANALYZE (★★★)

< h115
hint: EXPLAIN query, EXPLAIN ANALYZE query

< a115
-- Show query plan
EXPLAIN SELECT * FROM employees WHERE department = 'Engineering';

-- Show query plan with execution stats
EXPLAIN ANALYZE SELECT * FROM employees e
JOIN departments d ON e.department = d.name;

-- Analyze specific optimization
PRAGMA explain_output = 'all';
EXPLAIN SELECT * FROM employees WHERE salary > 70000;

< q116
Create and use sequences (★★★)

< h116
hint: CREATE SEQUENCE, nextval()

< a116
-- Create sequence
CREATE SEQUENCE emp_id_seq START 100;

-- Use sequence
SELECT nextval('emp_id_seq') AS next_id;
SELECT nextval('emp_id_seq') AS next_id;

-- Use in INSERT
CREATE TABLE new_employees (
    id INTEGER DEFAULT nextval('emp_id_seq'),
    name VARCHAR
);
INSERT INTO new_employees (name) VALUES ('Grace');
SELECT * FROM new_employees;

< q117
Use transactions for data integrity (★★★)

< h117
hint: BEGIN TRANSACTION, COMMIT, ROLLBACK

< a117
-- Start transaction
BEGIN TRANSACTION;

-- Make changes
UPDATE employees SET salary = salary * 1.1 WHERE department = 'Engineering';

-- Check results
SELECT * FROM employees WHERE department = 'Engineering';

-- Commit or rollback
COMMIT;
-- or ROLLBACK; to undo changes

< q118
Use COPY with advanced options (★★★)

< h118
hint: COPY with FORMAT, COMPRESSION, PARTITION_BY

< a118
-- Copy with all options
COPY (
    SELECT * FROM employees WHERE salary > 60000
) TO 'high_earners.parquet' (
    FORMAT PARQUET,
    COMPRESSION ZSTD,
    ROW_GROUP_SIZE 100000
);

-- Copy to stdout as CSV
COPY (SELECT * FROM employees LIMIT 3) TO '/dev/stdout' WITH (FORMAT CSV, HEADER);

-- Copy with partitioning
COPY employees TO 'by_dept' (FORMAT PARQUET, PARTITION_BY department);

< q119
Use prepared statements (★★★)

< h119
hint: PREPARE, EXECUTE, DEALLOCATE

< a119
-- Prepare a statement
PREPARE emp_by_dept AS 
SELECT * FROM employees WHERE department = $1 AND salary > $2;

-- Execute with parameters
EXECUTE emp_by_dept('Engineering', 70000);
EXECUTE emp_by_dept('Sales', 60000);

-- Deallocate when done
DEALLOCATE emp_by_dept;

< q120
Combine multiple advanced features in a complex query (★★★)

< h120
hint: CTEs, window functions, QUALIFY, complex types

< a120
WITH 
-- Calculate department statistics
dept_stats AS (
    SELECT 
        department,
        AVG(salary) AS avg_salary,
        STDDEV(salary) AS std_salary,
        LIST(name ORDER BY salary DESC) AS employees_by_salary
    FROM employees
    GROUP BY department
),
-- Rank employees within departments
ranked_employees AS (
    SELECT 
        e.*,
        ds.avg_salary AS dept_avg,
        ds.employees_by_salary,
        ROW_NUMBER() OVER (PARTITION BY e.department ORDER BY e.salary DESC) AS dept_rank,
        PERCENT_RANK() OVER (ORDER BY e.salary) AS company_percentile
    FROM employees e
    JOIN dept_stats ds ON e.department = ds.department
)
SELECT 
    name,
    department,
    salary,
    dept_avg,
    ROUND(company_percentile * 100, 1) AS percentile,
    dept_rank,
    CASE 
        WHEN dept_rank = 1 THEN 'Top Earner'
        WHEN company_percentile > 0.75 THEN 'High Performer'
        ELSE 'Standard'
    END AS classification,
    employees_by_salary[1] AS dept_top_earner
FROM ranked_employees
QUALIFY dept_rank <= 2
ORDER BY department, dept_rank;
